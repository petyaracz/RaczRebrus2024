---
title: "SI. The linking vowel should be lexically specified: Evidence from Hungarian"
author: "Rácz, Péter & Rebrus, Péter"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}

# -- setup -- #

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, fig.path = 'figures/', fig.width = 8, fig.height = 8)
# set wd in md
knitr::opts_knit$set(root.dir = '~/Github/RaczRebrus2024/')
setwd('~/Github/RaczRebrus2024/')

set.seed(1338)
options(knitr.kable.NA = "")

# -- pack -- #

library(tidyverse)
library(glue)
library(magrittr)
library(patchwork)
library(ggthemes)
library(gghalves)
library(knitr)

# -- fun -- #

getPLogOdds = function(dat, grouping_factor){
  dat %>% 
    group_by(across({{grouping_factor}})) %>%
    summarise(
      back = sum(back, na.rm = TRUE),
      front = sum(front, na.rm = TRUE)
    ) %>% 
    mutate(
      log_odds_back = log((back+1)/(front+1)),
      p_back = back/(back+front)
    )
}

transcribeIPA = function(string){
  stringr::str_replace_all(string, c(
      'ccs' = 'cscs', 'ssz' = 'szsz', 'zzs' = 'zszs', 'tty' = 'tyty', 'ggy' = 'gygy', 'nny' = 'nyny', 'lly' = 'jj', 'cs' = 'tʃ', 'sz' = 'ß', 'zs' = 'ʒ', 'ty' = 'G', 'gy' = 'ɟ', 'ny' = 'ɲ', 'ly' = 'j', 's' = 'ʃ', 'ß' = 's', 'a' = 'ɒ', 'á' = 'aː', 'e' = 'ɛ', 'í' = 'iː', 'ú' = 'uː', 'ü' = 'y', 'ű' = 'yː', 'ó' = 'oː', 'ő' = 'øː', 'ö' = 'ø', 'tt' = 'tː', 'c' = 'ts', 'G' = 'c', 'kk' = 'kː', 'gg' = 'gː', 'rr' = 'rː', 'ch' = 'tʃ', 'x' = 'ks', 'pp' = 'pː', 'tsts' = 'tsː', 'll' = 'lː', 'ff' = 'fː', 'dodʒɛm' = 'dodʒːɛm', 'bɒrtshɛs' = 'bɒrxɛs', 'tshɒrtɛr' = 'tʃɒrtɛr', 'tsodɛts' = 'kodɛk', 'burlɛsk' = 'børlɛsk', 'rotskɛr' = 'rokːɛr', 'mɒtɛr' = 'maːtɛr', 'kotːɛr' = 'koːtɛr', 'kɒdɛt' = 'kɒdɛtː'))
}

transcribe = function(string){
  stringr::str_replace_all(string, c(
      'ccs' = 'cscs', 'ssz' = 'szsz', 'zzs' = 'zszs', 'tty' = 'tyty', 'ggy' = 'gygy', 'nny' = 'nyny', 'lly' = 'jj', 'cs' = 'č', 'sz' = 'ß', 'zs' = 'ž', 'ty' = 'ṯ', 'gy' = 'ḏ', 'ny' = 'ṉ', 'ly' = 'j', 's' = 'š', 'ß' = 's'))
}

# breaks

pbreaks = c(.01,.05,.1,.25,.5,.75,.9,.95,.99)

# -- read -- #

# filtered AE stems with 30 most frequent suffixes, one suffixed form per row: 
# bojler; bojlertóől
# bojler; bojlertől
l = read_tsv('dat/dat_long.tsv')
# same, but one pair of back front suffixes per row
# bojler; bojlertól / bojlertől
w = read_tsv('dat/dat_wide.tsv')
# same, varying only, best knn preds
k = read_tsv('dat/dat_wide_knn.tsv')
# only stems
s = read_tsv('dat/dat_wide_stems.tsv')
# only suffixes
u = read_tsv('dat/dat_wide_suffixes.tsv')
# compact
c = read_tsv('dat/dat_wide_compact.tsv')
# glmm table
m = read_tsv('dat/glmm_comparison.tsv')
# glmmms
load('models/fit1b.rda')
load('models/fit3d.rda')

# -- munging -- #

# get postag ranking from freq
suffix_levels_freq = u %>%
  mutate(suffix = fct_reorder(suffix, -suffix_freq)) %>%
  pull(suffix) %>%
  levels()

# get postag ranking from p(back)
suffix_levels_back = w %>%
  summarise(
    back = sum(back),
    front = sum(front),
    .by = suffix
  ) %>% 
  mutate(
    odds = back/front,
    suffix = fct_reorder(suffix, -odds)
  ) %>% 
  pull(suffix) %>%
  levels()

# only keep varying forms
w2 = w %>% filter(form_varies)
s2 = s %>% filter(stem_varies)
c2 = c %>% filter(stem_varies)

```

## Research Questions

We have two research questions:

1. How much can we predict back/front variation based on the stem alone?
2. Can we improve on our predictions if we also include the suffixes?

In order to address these questions, we compiled a dataset of variable Hungarian back vowel + \<e\> noun stems and built a K Nearest-Neighbour model to categorise them using stem similarity. We then went on to add suffix-evel information to the model to see if this improved its accuracy.

## Dataset

We compiled a frequency list from the Hungarian Webcorpus 2 ([Nemeskey 2020](https://hlt.bme.hu/en/resources/webcorpus2)). The Webcorpus contains 1.8e+07 types and 8e+09 tokens. We filtered the frequency list to include noun forms of two syllables with a back vowel + \<e\>. We used a spellchecker (Ooms 2022) and hand-filtering to winnow the list. We picked the 30 most common suffix types that co-occur with these nouns. The resulting list has `r length(unique(l$stem))` stems and `r nrow(l)` suffixed forms. All stems are consonant-final.

A sample of the data for the stem _fotel_ (armchair) can be seen in Table 1.

```{r dat1}
# take long data, pick out fotel, order suffixes across suffix freq, get first x examples
l %>% 
  filter(stem == 'fotel') %>% 
  mutate(suffix = fct_relevel(suffix, suffix_levels_freq)) %>% 
  select(suffix,form,freq,examples) %>% 
  arrange(suffix,-freq) %>% 
  slice(1:17) %>% 
  rename(`suffix examples` = examples) %>%
 kable('simple', caption = glue('Table 1: Sample long data for _dzsungel_'))

```

We restricted the data to suffixed forms that do show back / front variation in the corpus, resulting in `r nrow(s2)` stems and `r nrow(w2)*2` suffixed forms. We went on to calculate the log odds ratio of back and front forms for each suffixed form (`log( back / front )`), resulting in `r nrow(w2)` suffixed pairs across `r length(unique(w2$stem))` stems. The difference here arises because `r nrow(s2) - length(unique(w2$stem))` stems do vary but only across, not within suffixes. 

A sample of the resulting data for _fotel_ can be seen in Table 2.

```{r dat2}
# take wide data, pick out dzsungel, order suffixes across suffix freq, get first x examples
w2 %>% 
  filter(
    stem == 'fotel'
         ) %>% 
  mutate(
    suffix = fct_relevel(suffix, suffix_levels_freq),
    log_odds_back = log( back / front )
         ) %>% 
  arrange(-log_odds_back) %>% 
  select(suffix,back,front,log_odds_back) %>% 
  slice(1:10) %>% 
  kable('simple', caption = glue('Table 2. Sample wide data for _fotel_'), digits = 2)

```


```{r dat3}
ranint_mod = lme4::glmer(cbind(back,front) ~ 1 + (1|stem) + (1|suffix), data = w, family = binomial) # all stems, for now!

ranint_stems = lme4::ranef(ranint_mod)$stem %>% 
  rownames_to_column('stem') %>% 
  right_join(s2)

r_stems = with(ranint_stems, cor(`(Intercept)`,log_odds_back))

```

We sum front / back counts across stems and calculate log odds. An alternative would be to take into account the hierarchy in the data and estimate a random intercept for stems using a hierarchical binomial model following Janda, Nesset, and Baayen (2010). The correlation between the random intercepts and the raw log odds is `r round(r_stems, 2)`.

## K Nearest-Neighbours Model

We took the `r nrow(s2)` varying stems and transcribed them using a simplified phonetic transcription. This transcription replaced letter digraphs with single characters (_szatyor_ "bag" -> \<saṯor\>). We calculated the log odds of back / front forms for each stem by grouping the data across stems and summing back and front counts across suffixes. We split the stems into five frequency quantiles.

Our K-Nearest Neighbour learner was written in R. It matched a target word to test words and predicted its behaviour based on the behaviour of its nearest neighbours. It calculated the Levenshtein distance between the transcribed test word and transcribed target words, arranged target words from smallest to largest distance from the test word, and selected the first k target words. Some target words might have the same Levenshtein distance from the test word (e.g. the Levenshtein distance between _hotel_, _motel_, and _fotel_ "armchair" is 1), so the order of target words within distance brackets was randomised. The learner then summed over the back and front form counts for the k nearest neighbours and calculated a total log odds. The learner returned this value as the prediction for the test form. The learner used a leave-one-out fitting method, comparing test forms to all training forms except the test form itself.

An example with `k = 3` and the target word _fotel_ can be seen in Table 3. The first columns shows the test word. The second column shows the five closest neighbours to _fotel_: _hotel, motel, totem, notesz, fater_. The summed back and front counts for each form are in the next two columns (viz. there are, in total, 7722 back variants and 140832 front variants of _hotel_ in the dataset). Given `k = 3`, we sum over back and front forms for the first three and log the odds, which is `log(7722+623+3=8348/140832+3386+142=144360) = -2.58` (or p = .07). This is the predicted value for _fotel_. The true value is -.76 (p = .32), so the learner is not particularly accurate in this example.

|test  |target |transcription | back | front | distance|k  |sum back | sum front |  pred|
|:-----|:------|:-------------|-----:|------:|--------:|--:|--------:|----------:|-----:|
|fotel |hotel  |hotel         |  7722| 140832|        1|  1|         |           |      |
|      |motel  |motel         |   623|   3386|        1|  2|         |           |      |
|      |totem  |totem         |     3|    142|        2|  3|     8348|     144360| -2.58|
|      |notesz |notes         |   628|   3686|        2|  4|         |           |      |
|      |fater  |fater         |  3133|    190|        2|  5|         |           |      |

Table 3. Example KNN for _fotel_.

Our learner differs from KNN learner used in categorisation problems and machine learning. A more typical KNN learner provides a category label, not a category weight. In addition, a more typical KNN learner will not involve a random component, since distances in any given category space are likely more fine-grained and so unique for every target item in the training set. (Think of an RGB scale on which every unit of change in R/G/B from a reference colour will define a distinct colour, however small the difference is.) 

Our learner had two parameters, `k`, the number of nearest neighbours (possible values: 1,7,10,12,15), and `f`, the relative frequency of stems in the training set (possible values: 1-5, where the training set consists of forms in the f+ quantiles of the total training set).

For each parameter setting, we fit a binomial generalised linear model predicting the back/front ratio for each stem from the KNN prediction for that stem across all `r nrow(c2)` stems. We used the linear model's z-value to select the best model. Since models only differed from one another by the KNN parameter settings, this gave us the best KNN parameter settings: `k = 7` and `f = 3`. This means that the best learner compared the target form to its first seven nearest neighbours. The best learner operated on the top 40% of the log odds distribution of training forms, ignoring the less frequent training forms.

## Combined model

In order to incorporate suffix-specific information on some level, we marked whether a suffix was consonant- or vowel-initial in our paired dataset. That is, whether the suffix involved a linking vowel. We then went on to build generalised linear mixed models that used stem-level information, KNN predictions, and suffix-level information, presence of a linking vowel, to predict the log odds of variable forms in the data.

We built four models, shown in Table 4. We used AIC, BIC, and a likelihood ratio test of model fit to find the best random effect structure for each model and to find the best model.

```{r glmms}
m %>% 
  kable('simple',digits = 2, caption = 'Table 4. Models of the variable back + e pairs.')

```

We go through the models one by one.

- Model 1 is our reference model. It groups data across stem and suffix. 
- Model 2 includes the stem-level predictions of the best KNN learner. AIC and BIC show that learner predictions improve model fit over the null model.
- Model 3 adds suffix-level information to the formula, viz. whether the suffix is consonant- or vowel-initial.
- Model 4 tests the interaction of suffix-initial vowel and stem-level learner prediction. This model provides **the best fit** of the data.

## Hayes Check

If the stem ends in one of the categories from Hayes et al 2009, do we see the same pattern in our corpus data as they saw in their data (preference for front variants)? Yes:

```{r hayes, eval = T}
# - the stem ends in a bilabial noncontinuant ([p, b, m]).
# - the stem ends in a sibilant ([s, z, š, ž, č, dž, ts]).
# - the stem ends in a coronal sonorant ([n, ṉ, l, r]).
# - the stem ends in a sequence of two consonants.

s2b = s2 %>% 
  mutate(
    transcription = transcribe(stem),
    ends_labial_stop =  str_detect(transcription, '[pbm]$'),
    ends_sibilant =  str_detect(transcription, '[szšžč]$'),
    ends_coronal_sonorant = str_detect(transcription, '[nṉlr]$'),
    ends_two_consonants = str_detect(transcription, 'e[^e]{2}$'),
    ends_something = ends_labial_stop | ends_sibilant | ends_coronal_sonorant | ends_two_consonants
  )

# s2b %>% 
  # count(ends_something)

glm_hayes = glm(cbind(back,front) ~ 1 + ends_something, data = s2b, family = binomial)
broom::tidy(glm_hayes, conf.int = T) %>% 
  kable(digits = 2)

```

## Visualisations

### Figure 1

```{r fig1, dpi = 500, fig.width = 8, fig.height = 6}

p1 = w %>% 
  getPLogOdds(c(stem,suffix_initial)) %>% 
  mutate(
    suffix_initial2 = glue('{suffix_initial}-initial\nsuffix'),
    `stem type` = case_when(
      back == 0 ~ 'true neutral',
      front ==0 ~ 'disharmonic',
      T ~ 'vacillating'
    )
         ) %>% 
  ggplot(aes(suffix_initial2,fill = `stem type`)) +
  geom_bar() +
  coord_flip() +
  theme_few() +
  xlab('') +
  ylab('count') +
  scale_fill_colorblind() +
  ggtitle('Ratios of stem classes')

p2 = w %>% 
  filter(form_varies) %>% 
  getPLogOdds(c(stem,suffix_initial)) %>% 
  mutate(suffix_initial2 = glue('{suffix_initial}-initial\nsuffix')) %>% 
  ggplot(aes(suffix_initial2,log_odds_back)) +
  geom_half_violin(side = 'r') +
  geom_half_boxplot(width = .1, side = 'r') +
  geom_half_point(width = .6, side = 'l') +
  geom_hline(yintercept = c(qlogis(.01),qlogis(.99)), lty = 3) +
  geom_hline(yintercept = 0, lty = 2) +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = pbreaks, name = 'p(back)'), limits = c(-6,6), name = 'log (back / front)', breaks = c(-5:5)) +
  xlab('') +
  coord_flip() +
  theme_bw() +
  ggtitle('Ratios across vacillating stems')

p1 / p2 + plot_layout(heights = c(1,2))
   
```

### Figure 2

```{r fig2, dpi = 500, fig.width = 6, fig.height = 12}

p3 = c %>% 
  filter(stem_varies_c,stem_varies_v) %>% 
  distinct(stem,v_minus_c,log_odds_back_c,log_odds_back_v) %>%
  # add nice names
  # set up two arbitrary categories for faceting so it's easier to see
  mutate(
    stem2 = transcribeIPA(stem),
    category = case_when(
      v_minus_c > 0 ~ 'C < V',
      v_minus_c <= 0 ~ 'C > V'
    ) %>% 
      fct_relevel('C > V') # V takes the lead
  ) %>% 
  rename(
    'consonant-initial' = log_odds_back_c,
    'vowel-initial' = log_odds_back_v
  ) %>% 
  # stretch data for plotting
  pivot_longer(-c(stem,stem2,v_minus_c,category)) %>% 
  # plot each stem twice: once for vowel-initial intercept, once for consonant-initial ~. 
  # jitter stem label a bit so it's seen.
  # add lines between the two data points per stem
  # split into two panels so it's easier to see.
  # add sec axis for p again
  ggplot(aes(name,value,label = stem2,group = stem2)) +
  geom_line() +
  geom_label(position = position_jitter(height = 0, width = .1)) +
  theme_bw() +
  facet_wrap( ~ category) +
  xlab('') +
  geom_hline(yintercept = c(qlogis(.01),qlogis(.99)), lty = 3) +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = pbreaks, name = 'p (back)'), name = 'log (back/front)') +
  ggtitle('Overall preference for back/front forms\nacross stems and suffix type')

p4 = c %>% 
  mutate(stem2 = transcribeIPA(stem)) %>% 
  filter(stem_varies_c,stem_varies_v) %>% 
  ggplot(aes(log_odds_back_stem,v_minus_c)) +
  geom_label(aes(label = stem2)) +
  geom_smooth() +
  theme_few() +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = c(qlogis(.01),qlogis(.99)), lty = 3) +
  ylab('(log (back/front) V + 10) -\n(log(back/front) C + 10)') +
  scale_x_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = pbreaks, name = 'p (back)'), name = 'log (back/front)', breaks = c(-5:5)) +
  ggtitle('Difference of preference for back/front forms\nacross suffix type')

p3 / p4 + plot_layout(heights = c(2,1))

```

### Figure 3

```{r fig3, dpi = 500, fig.width = 6, fig.height = 3}

k2 = k %>% 
  filter(form_varies) %>% 
  group_by(stem,knn) %>% 
  summarise(
    back = sum(back),
    front = sum(front)
  ) %>% 
  mutate(
    log_odds_back = log(back/front)
  )

# with(k2, cor(knn,log_odds_back))

k2 %>% 
  ggplot(aes(log_odds_back,knn)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  theme_bw() +
  # xlab('log (back / front)') +
  # ylab('KNN weight') +
  ggtitle('Stems across variable forms (log odds)') +
  geom_vline(xintercept = c(qlogis(.01),qlogis(.99)), lty = 3) +
  geom_hline(yintercept = c(qlogis(.01),qlogis(.99)), lty = 3) +
  scale_x_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = pbreaks, name = 'p (back)'), name = 'log (back/front)', breaks = c(-5:5)) +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = pbreaks, name = 'KNN: p (back)'), name = 'KNN: log (back/front)', breaks = c(-5:5))
# https://media1.tenor.com/m/MYHn1AuShTcAAAAC/face-melt.gif

```

### Figure 4

```{r fig4, dpi = 500, fig.width = 7, fig.height = 4}

sjPlot::plot_model(fit3d, 'pred', terms = c('knn', 'suffix_initial')) +
  theme_bw() +
  labs(colour = 'suffix-initial') +
  scale_x_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = c(.05,.25,.50,.75,.95), name = 'knn p(back)'), breaks = -4:4, name = 'knn log (back/front)') +
  scale_y_continuous(sec.axis = sec_axis(trans = ~ qlogis(.), breaks = -4:4, name = 'combined model log(back/front)'), breaks = pbreaks, name = 'combined model p(back)') +
  theme_bw() +
  scale_color_colorblind() +
  scale_fill_colorblind() +
  ggtitle('')

```

### Figure 5

```{r fig5, dpi = 500, fig.width = 10, fig.height = 5}

ety = read_tsv('dat/stemlanguage.tsv')

ety %<>%
  group_by(language) %>% 
  mutate(
    n = n(),
    origin = case_when(
  n <= 10 ~ 'other',
  n > 10 ~ language
    ) %>% 
  factor(levels = c('de', 'en', 'fr', 'yi', 'la', 'other'))
  )

c %>% 
  left_join(ety) %>% 
  mutate(stem2 = transcribeIPA(stem)) %>% 
  filter(stem_varies_c,stem_varies_v) %>% 
  ggplot(aes(log_odds_back_stem,v_minus_c,label = stem2)) +
  geom_label(aes(colour = origin)) +
  geom_smooth() +
  theme_few() +
  geom_vline(xintercept = c(qlogis(.01),qlogis(.99)), lty = 3) +
  ylab('(log (back/front) V + 10) - (log(back/front) C + 10)') +
  scale_x_continuous(sec.axis = sec_axis(trans = ~ plogis(.), breaks = pbreaks, name = 'p (back)'), name = 'log (back/front)', breaks = c(-5:5)) +
  scale_colour_viridis_d(option = 'H')
# +
  #ggtitle('Figure 4. Suffix patterns in the data II.')


```